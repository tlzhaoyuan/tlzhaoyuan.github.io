---
layout:     post
title:      Pytorch源码阅读之自动微分引擎概要
subtitle:   自动微分（AutoDiff）是深度学习框架最核心的功能。AutoGrad是Pytorch自动微分的实现。
date:       2022-06-03
author:     Yuan Zhao
header-img: img/bg-default.jpg
catalog: true
tags:
    - Pytorch
    - 深度学习
    - C++
---
# Pytorch源码阅读之自动微分引擎概要

## 计算图基础
![](https://openmlsys.github.io/_images/simpledag.svg)
## Pytorch计算图实现
### `Variable`
`Variable`是Pytorch中神经网络数据的容器，可以用来装参数、输入、输出和中间变量
### `AutogradMeta`
持有反向求导时需要的求导方式、梯度聚合方式以及求导/聚合的梯度结果
#### 主要字段
* `Variable grad_`:存储梯度
* `share_ptr<Node> grad_fn_`: 梯度计算的求导函数，对于非叶子节点，需要接受后续节点回传回来的梯度，并根据求导函数计算该节点往前回传的梯度
* `weak_ptr<Node> grad_accumulator_`: 梯度聚合方式，叶子节点是梯度回传的终点，其没有求导函数，但需要将不同节点回传回来的梯度进行聚合，以更新叶子节点的参数值

### `Node`
#### 主要字段
* `edge_list next_egdes`: 反向求导时节点的输出边，即前向计算时节点的输入

#### 主要方法
* `variable_list operator()(variable_list&& inputs)`：重载了`（）`，将输入转发给`apply`进行计算
* `virtual variable_list apply(variable_list&& inputs)`: protected，不同类型节点定义自己的求导方式
之前叫Function，定义了一个Op需要做的事

### `Edge`
* `shared_ptr<Node> function`: 反向求导时该边指向的节点，即前向传播时function节点有一条输出边是该边
* `uint32_t input_nr`: 该边是前向传播时function节点的第几条输出边，也就是反向求导时求导函数的第几个输入。这块可能有点绕，就是前向传播时一个OP可能有多个输出，在反向求导时该OP节点的求导函数就需要接受多个边回传的梯度，`input_nr`就是用来区分（前向传播时）不同输出边的。

### `GraphTask`
一个自动微分的计算任务，由很多NodeTask构成

#### 主要字段
* `unordered_map<Node*, InputBuffer> not_ready_`: 计算图中节点和其输入的映射 
* `unordered_map<Node*, int> dependencies_`: 计算图中节点依赖的未完成的输入边数，实时更新
* 线程同步的锁和条件变量


### `NodeTask`
计算任务的最小单元，即一个Op和其输入值。`NodeTask`封装了OP和其输入，时机成熟可以直接执行

#### 主要字段
* `GraphTask* base_`: 该`NodeTask`隶属的`GraphTask`
* `shared_ptr<Node> fn_`：该task的计算方式
* `InputBuffer inputs_`：该task的输入值


### `ReadyQueue`
线程安全的队列，支持push和pop（队列为空时会阻塞），里面装的是NodeTask实例

#### 主要字段
* `priority_queue<NodeTask> heap_`
* 线程同步的锁和条件变量

### `Engine`
自动微分引擎，单例的，一个Pytorch进程只有一个Engine